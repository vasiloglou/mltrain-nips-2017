{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BDQN (Bayesian-Deep Q-Network)\n",
    "### A Thompson Sampling of deep explorative RL\n",
    "This tutorial walks through the implementation of Bayesian deep Q networks (BDQNs), \n",
    "an RL method which applies the function approximation capabilities of deep neural networks\n",
    "to problems in reinforcement learning.\n",
    "The model in this tutorial follows the work described in the paper \n",
    "[Efficient Exploration through Bayesian Deep Q-Networks](Under Review), written by Kamyar Azizzadenesheli, Emma Brunskil and, Anima Anankumar.\n",
    "\n",
    "To keep these toturial runnable \n",
    "by as many people as possible, \n",
    "on as many machines as possible,\n",
    "and with as few headaches as possible, \n",
    "we have so far avoided dependencies on external libraries \n",
    "(besides mxnet, numpy and matplotlib). \n",
    "However, in this case, we'll need to import the [OpenAI Gym](https://gym.openai.com/docs).\n",
    "That's because in reinforcement learning, \n",
    "instead of drawing examples from a data structure, \n",
    "our data comes from interactions with an environment. \n",
    "In this chapter, our environemnts will be classic Atari video games.\n",
    "\n",
    "## Preliminaries\n",
    "The following code clones and installs the OpenAI gym.\n",
    "`git clone https://github.com/openai/gym ; cd gym ; pip install -e .[all]` \n",
    "Mosly users of AWS EC2 instance need an additional command to be run before intallation:\n",
    "\n",
    "`apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
    "`\n",
    "\n",
    "Full documentation for the gym can be found on [at this website](https://gym.openai.com/).\n",
    "If you want to see reasonable results before the sun sets on your AI career,\n",
    "we suggest running these experiments on a server equipped with GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import gym\n",
    "import math\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import pickle\n",
    "import logging, logging.handlers\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "command = 'mkdir data' # Creat a direcotry to store models and scores.\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the algorithm\n",
    "The BDQN, in the sense of implementation, is same as DDQN, [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461), written by Hado van Hasselt, except in the last layer, where instead of using linear regression as in DDQN, BDQN uses Bayesian Linear Regression (BLR) and for exploration, instead of using naive $\\varepsilon$-greedy strategy as in DDQN, BDQN uses Thompson sampling and avoid any naive exploration.\n",
    "\n",
    "As it is mentioned before, BDQN has the same architecture as DDQN has except, in BDQN we remove the last layer of DDQN. We call the output of the network as a representation $\\phi(\\cdot)$, and instead assign BLR layer on the top of the representation. The input to the network is state of the environment, `x` and the output is $\\phi(x)$, the feature representation. The input to BLR block is the feature representation.\n",
    "\n",
    "|![](./images/Alg.jpeg)|\n",
    "|:---------------:|\n",
    "|BDQN Algorithm|\n",
    "\n",
    "#### BDQN Architecture\n",
    "The input to the network part of BDQN is 4 × 84 × 84 tensor with a\n",
    "rescaled, mean-scale version of the last four observations. The first convolution layer has 32 filters of size 8 with a stride of 4. The second convolution layer has 64 filters of size 4 with stride 2. The last convolution layer has 64 filters of size 3 followed by a fully connected layers with size 512. We add a BLR layer on top of this.\n",
    "\n",
    "|![](./images/net.jpeg)|\n",
    "|:---------------:|\n",
    "|BDQN Architecture|\n",
    "\n",
    "#### BLR, a closed form way of computing posterior. \n",
    "In both DDQN (linear regression) and BDQN (Bayesian linear regression) the common assumptions are as follows:\n",
    "\n",
    "* The layer before the last layer provides features $\\phi(\\cdot)$, suitable for linear models.\n",
    "* The generative model for state-action value, Q(x,a) is drawn from the following generative model:\n",
    "$$y\\sim w_a\\phi(x)+\\epsilon$$\n",
    "Where $y$ is a sample of Q(x,a) and for simplicity we assume $\\epsilon$ is a mean-zero Gaussian noise with variance $\\sigma_n^2$. \n",
    "\n",
    "The question in linear regression problem is given a bunch of (x,a,y), what $w_a$ can be in term of minimizing least square error and the task is to find a $w_a$ which matches $x,a$ to $y$. In Bayesian machinery, we assume $w_a$ is drawn from a prior distribution, e.g. mean-zero Gaussian distribution with variance $\\sigma^2$. Given data, the question in BLR is what is the posterior distribution of $w_a$ which matches $x,a$ to $y$. The interesting property of BLR is that given data, the distribution of $w_a$, therefore Q(a,x), can be computed in closed form and due to the conjugacy, the distribution over of samples of Q(a,x) has closed form. \n",
    "\n",
    "Given this nice property, at each time step, we can compute the posterior distribution of Q-function. As Thompson Sampling based strategies suggest, we draw a Q-function out of the posterior distribution and act optimally with respect to that for that time step.\n",
    "#### Target Network\n",
    "\n",
    "In both DDQN and BDQN we assume $\\phi_\\theta(\\cdot)$ is paramterzied by parameters $\\theta$. Furthermore, the obervation $y$ for time step $t$ is after seeing the cosequative seqeucen of $x_t,a_t,r_t,x_{t+1}$\n",
    "$$y_t := r_t + \\lambda Q^{target}(x_{t+1},argmax_{a'}Q(x_{t+1},a'))$$\n",
    "Where $Q^{target}$ has same structure of Q, but with paramteters $\\theta^{target}$ and $w_a^{target},~\\forall{a\\in\\mathcal{A}}$ where the target parameters get updated ones in a while.\n",
    "\n",
    "#### Posterior update\n",
    "\n",
    "Given a dataset $\\mathcal{D}=\\lbrace x_i,a_i,y_i\\rbrace_{i=1}^D$, we construct $|\\mathcal{A}|$ disjoint datasets for each action, $\\mathcal{D}=\\cup_{a\\in\\mathcal{A}} \\mathcal{D}_a$ where $\\mathcal{D}_a$ is a set of tuples $x_i,a_i,y_i$ with the action $a_i = a$ and size $D_a$. Given the data set $\\mathcal{D}_a$, we are interested in $\\mathbb{P}(w_a|\\mathcal{D}_a)$ and $\\mathbb{P}(Q(x,a)|\\mathcal{D}_a),\\forall x\\in\\mathcal{X}$. Let us construct a matrix $\\Phi_a\\in\\Re^{d\\times D_a}$, a concatenation of feature column vectors $\\lbrace\\phi(x_i)\\rbrace_{i=1}^{D_a}$, and $\\textbf{y}_a\\in\\Re^{D_a}$, a concatenation of target values in set $\\mathcal{D}D_a$. Therefore the posterior distribution is as follows:\n",
    "\n",
    "$$w_a\\sim \\mathcal{N}\\left(\\frac{1}{\\sigma^2_\\epsilon}\\Xi_a\\Phi_a\\textbf{y}_a ,\\Xi_a\\right),~~\\textit{where}~~\\Xi_a = \\left(\\frac{1}{\\sigma_\\epsilon^2}\\Phi_a\\Phi_a^\\top+\\frac{1}{\\sigma^2}I \\right)^{-1}$$\n",
    "and since the prior and likelihood are conjugate of each other we have\n",
    "$$\n",
    "Q(x,a)|\\mathcal{D}_a\\sim \\mathcal{N}\\left(\\frac{1}{\\sigma^2_\\epsilon}\\phi(x)^\\top\\Xi_a\\Phi_a\\textbf{y}_a ,\\phi(x)^\\top\\Xi_a\\phi(x)\\right)$$\n",
    "where $I\\in\\Re^d$ is a identity matrix. The \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Collect samples\n",
    "At the beginning of each episode (one round of the game), \n",
    "reset the environment to its initial state using `env.reset()`. \n",
    "At each time step $t$, the environment is at `current_state`.\n",
    "Given the seen data up to time $t$, the agnet update the posteriro distribution of Q-functions and deploys Thompson Sampling in order to choose the action $a_{TS}$.\n",
    "\n",
    "Pass the action through `env.step(action)` to receive next frame, reward and whether the game terminates.\n",
    "Append this frame to the end of the `current_state` and construct `next_state` while removeing $frame(t-12)$.\n",
    "Store the tuple $( `current_state` $, action, reward, `next_ state` )$ in the replay buffer.\n",
    "\n",
    "#### Update Network\n",
    "* Draw batches of tuples from the replay buffer: $(\\phi,r,a,\\phi')$.\n",
    "* Update $\\theta$ using the following loss:\n",
    "$$\\Large(\\small Q(W,a,\\theta)-r-Q(W,argmax_{a'}Q(\\phi',a',\\theta),\\theta^{target})\\Large)^2$$\n",
    "where $W$ is the set of $w_a,~a\\in\\forall \\mathcal{A}$\n",
    "* Update the $\\theta$\n",
    "* Update the $\\theta^{target}$ once in a while\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        #Articheture\n",
    "        self.batch_size = 32 # The size of the batch to learn the Q-function\n",
    "        self.image_size = 84 # Resize the raw input frame to square frame of size 80 by 80 \n",
    "        #Trickes\n",
    "        self.replay_buffer_size = 100000 # The size of replay buffer; set it to size of your memory (.5M for 50G available memory)\n",
    "        self.learning_frequency = 4 # With Freq of 1/4 step update the Q-network\n",
    "        self.skip_frame = 4 # Skip 4-1 raw frames between steps\n",
    "        self.internal_skip_frame = 4 # Skip 4-1 raw frames between skipped frames\n",
    "        self.frame_len = 4 # Each state is formed as a concatination 4 step frames [f(t-12),f(t-8),f(t-4),f(t)]\n",
    "        self.Target_update = 10000 # Update the target network each 10000 steps\n",
    "        self.epsilon_min = 0.1 # Minimum level of stochasticity of policy (epsilon)-greedy\n",
    "        self.annealing_end = 1000000. # The number of step it take to linearly anneal the epsilon to it min value\n",
    "        self.gamma = 0.99 # The discount factor\n",
    "        self.replay_start_size = 50000 # Start to backpropagated through the network, learning starts\n",
    "        \n",
    "        #otimization\n",
    "        self.max_episode =   200000000 #max number of episodes#\n",
    "        self.lr = 0.0025 # RMSprop learning rate\n",
    "        self.gamma1 = 0.95 # RMSprop gamma1\n",
    "        self.gamma2 = 0.95 # RMSprop gamma2\n",
    "        self.rms_eps = 0.01 # RMSprop epsilon bias\n",
    "        self.ctx = mx.gpu() # Enables gpu if available, if not, set it to mx.cpu()\n",
    "        self.lastlayer = 512 # Dimensionality of feature space\n",
    "        self.f_sampling = 1000 # frequency sampling E_W_ (Thompson Sampling)\n",
    "        self.alpha = .01 # forgetting factor 1->forget\n",
    "        self.alpha_target = 1 # forgetting factor 1->forget\n",
    "        self.f_bayes_update = 1000 # frequency update E_W and Cov\n",
    "        self.target_batch_size = 5000 #target update sample batch\n",
    "        self.BayesBatch = 10000 #size of batch for udpating E_W and Cov\n",
    "        self.target_W_update = 10\n",
    "        self.lambda_W = 0.1 #update on W = lambda W_new + (1-lambda) W\n",
    "        self.sigma = 0.001 # W prior variance\n",
    "        self.sigma_n = 1 # noise variacne\n",
    "opt = Options()\n",
    "\n",
    "env_name = 'AsterixNoFrameskip-v4' # Set the desired environment\n",
    "env = gym.make(env_name)\n",
    "num_action = env.action_space.n # Extract the number of available action from the environment setting\n",
    "\n",
    "manualSeed = 1 # random.randint(1, 10000) # Set the desired seed to reproduce the results\n",
    "mx.random.seed(manualSeed)\n",
    "attrs = vars(opt)\n",
    "\n",
    "# set the logger\n",
    "logger = logging.getLogger()\n",
    "file_name = './data/results_BDQN_%s_lr_%f.log' %(env_name,opt.lr)\n",
    "fh = logging.handlers.RotatingFileHandler(file_name)\n",
    "fh.setLevel(logging.DEBUG)#no matter what level I set here\n",
    "formatter = logging.Formatter('%(asctime)s:%(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "ff =(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "logging.error(str(ff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the feature representation model\n",
    "The network is constructed as three CNN layers and a fully connected added on the top. Furthermore, the optimizer is assigned to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DQN_gen():\n",
    "    DQN = gluon.nn.Sequential()\n",
    "    with DQN.name_scope():\n",
    "        #first layer\n",
    "        DQN.add(gluon.nn.Conv2D(channels=32, kernel_size=8,strides = 4,padding = 0))\n",
    "        DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "        DQN.add(gluon.nn.Activation('relu'))\n",
    "        #second layer\n",
    "        DQN.add(gluon.nn.Conv2D(channels=64, kernel_size=4,strides = 2))\n",
    "        DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "        DQN.add(gluon.nn.Activation('relu'))\n",
    "        #tird layer\n",
    "        DQN.add(gluon.nn.Conv2D(channels=64, kernel_size=3,strides = 1))\n",
    "        DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "        DQN.add(gluon.nn.Activation('relu'))\n",
    "        DQN.add(gluon.nn.Flatten())\n",
    "        #fourth layer\n",
    "        #fifth layer\n",
    "        DQN.add(gluon.nn.Dense(opt.lastlayer,activation ='relu'))\n",
    "    DQN.collect_params().initialize(mx.init.Normal(0.02), ctx=opt.ctx)\n",
    "    return DQN\n",
    "\n",
    "dqn_ = DQN_gen()\n",
    "target_dqn_ = DQN_gen()\n",
    "\n",
    "DQN_trainer = gluon.Trainer(dqn_.collect_params(),'RMSProp', \\\n",
    "                          {'learning_rate': opt.lr ,'gamma1':opt.gamma1,'gamma2': opt.gamma2,'epsilon': opt.rms_eps,'centered' : True})\n",
    "dqn_.collect_params().zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer\n",
    "Replay buffer store the tuple of : `state`, action , `next_state`, reward , done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward','done'))\n",
    "class Replay_Buffer():\n",
    "    def __init__(self, replay_buffer_size):\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.replay_buffer_size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.replay_buffer_size\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized BLR matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bat_state = nd.empty((1,opt.frame_len,opt.image_size,opt.image_size), opt.ctx)\n",
    "bat_state_next = nd.empty((1,opt.frame_len,opt.image_size,opt.image_size), opt.ctx)\n",
    "bat_reward = nd.empty((1), opt.ctx)\n",
    "bat_action = nd.empty((1), opt.ctx)\n",
    "bat_done = nd.empty((1), opt.ctx)\n",
    "\n",
    "eye = nd.zeros((opt.lastlayer,opt.lastlayer), opt.ctx)\n",
    "for i in range(opt.lastlayer):\n",
    "    eye[i,i] = 1\n",
    "\n",
    "E_W = nd.normal(loc=0, scale=.01, shape=(num_action,opt.lastlayer),ctx = opt.ctx)\n",
    "E_W_target = nd.normal(loc=0, scale=.01, shape=(num_action,opt.lastlayer),ctx = opt.ctx)\n",
    "E_W_ = nd.normal(loc=0, scale=.01, shape=(num_action,opt.lastlayer),ctx = opt.ctx)\n",
    "Cov_W = nd.normal(loc=0, scale= 1, shape=(num_action,opt.lastlayer,opt.lastlayer),ctx = opt.ctx)+eye\n",
    "Cov_W_decom = Cov_W\n",
    "for i in range(num_action):\n",
    "    Cov_W[i] = eye\n",
    "    Cov_W_decom[i] = nd.array(np.linalg.cholesky(((Cov_W[i]+nd.transpose(Cov_W[i]))/2.).asnumpy()),ctx = opt.ctx)\n",
    "Cov_W_target = Cov_W\n",
    "phiphiT = nd.zeros((num_action,opt.lastlayer,opt.lastlayer), opt.ctx)\n",
    "phiY = nd.zeros((num_action,opt.lastlayer), opt.ctx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLR posteriro update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = opt.sigma\n",
    "sigma_n = opt.sigma_n\n",
    "\n",
    "def BayesReg(phiphiT,phiY,alpha,batch_size):\n",
    "    phiphiT *= (1-alpha) #Forgetting parameter alpha suggest how much of the moment from the past can be used, we set alpha to 1 which means do not use the past moment\n",
    "    phiY *= (1-alpha)\n",
    "    for j in range(batch_size):\n",
    "        transitions = replay_memory.sample(1) # sample a minibatch of size one from replay buffer\n",
    "        bat_state[0] = transitions[0].state.as_in_context(opt.ctx).astype('float32')/255.\n",
    "        bat_state_next[0] = transitions[0].next_state.as_in_context(opt.ctx).astype('float32')/255.\n",
    "        bat_reward = transitions[0].reward \n",
    "        bat_action = transitions[0].action \n",
    "        bat_done = transitions[0].done \n",
    "        phiphiT[int(bat_action)] += nd.dot(dqn_(bat_state).T,dqn_(bat_state))\n",
    "        phiY[int(bat_action)] += (dqn_(bat_state)[0].T*(bat_reward +(1.-bat_done) * opt.gamma * nd.max(nd.dot(E_W_target,target_dqn_(bat_state_next)[0].T))))\n",
    "    for i in range(num_action):\n",
    "        inv = np.linalg.inv((phiphiT[i]/sigma_n + 1/sigma*eye).asnumpy())\n",
    "        E_W[i] = nd.array(np.dot(inv,phiY[i].asnumpy())/sigma_n, ctx = opt.ctx)\n",
    "        Cov_W[i] = sigma * inv\n",
    "    return phiphiT,phiY,E_W,Cov_W \n",
    "\n",
    "# Thompson sampling, sample model W form the posterior.\n",
    "def sample_W(E_W,U):\n",
    "    for i in range(num_action):\n",
    "        sam = nd.normal(loc=0, scale=1, shape=(opt.lastlayer,1),ctx = opt.ctx)\n",
    "        E_W_[i] = E_W[i] + nd.dot(U[i],sam)[:,0]\n",
    "    return E_W_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess frames\n",
    "* Take a frame, average over the `RGB` filter and append it to the `state` to construct `next_state`\n",
    "* Clip the reward\n",
    "* Render the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(raw_frame, currentState = None, initial_state = False):\n",
    "    raw_frame = nd.array(raw_frame,mx.cpu())\n",
    "    raw_frame = nd.reshape(nd.mean(raw_frame, axis = 2),shape = (raw_frame.shape[0],raw_frame.shape[1],1))\n",
    "    raw_frame = mx.image.imresize(raw_frame,  opt.image_size, opt.image_size)\n",
    "    raw_frame = nd.transpose(raw_frame, (2,0,1))\n",
    "    raw_frame = raw_frame.astype('float32')/255.\n",
    "    if initial_state == True:\n",
    "        state = raw_frame\n",
    "        for _ in range(opt.frame_len-1):\n",
    "            state = nd.concat(state , raw_frame, dim = 0)\n",
    "    else:\n",
    "        state = mx.nd.concat(currentState[1:,:,:], raw_frame, dim = 0)\n",
    "    return state\n",
    "\n",
    "def rew_clipper(rew):\n",
    "    if rew>0.:\n",
    "        return 1.\n",
    "    elif rew<0.:\n",
    "        return -1.\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def renderimage(next_frame):\n",
    "    if render_image:\n",
    "        plt.imshow(next_frame);\n",
    "        plt.show()\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(.1)\n",
    "        \n",
    "l2loss = gluon.loss.L2Loss(batch_axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame_counter = 0. # Counts the number of steps so far\n",
    "annealing_count = 0. # Counts the number of annealing steps\n",
    "epis_count = 0. # Counts the number episodes so far\n",
    "replay_memory = Replay_Buffer(opt.replay_buffer_size) # Initialize the replay buffer\n",
    "tot_clipped_reward = []\n",
    "tot_reward = []\n",
    "frame_count_record = []\n",
    "moving_average_clipped = 0.\n",
    "moving_average = 0.\n",
    "flag = 0\n",
    "c_t = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "render_image = False # Whether to render Frames and show the game\n",
    "batch_state = nd.empty((opt.batch_size,opt.frame_len,opt.image_size,opt.image_size), opt.ctx)\n",
    "batch_state_next = nd.empty((opt.batch_size,opt.frame_len,opt.image_size,opt.image_size), opt.ctx)\n",
    "batch_reward = nd.empty((opt.batch_size),opt.ctx)\n",
    "batch_action = nd.empty((opt.batch_size),opt.ctx)\n",
    "batch_done = nd.empty((opt.batch_size),opt.ctx)\n",
    "\n",
    "while epis_count < opt.max_episode:\n",
    "    cum_clipped_reward = 0\n",
    "    cum_reward = 0\n",
    "    next_frame = env.reset()\n",
    "    state = preprocess(next_frame, initial_state = True)\n",
    "    t = 0.\n",
    "    done = False\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        mx.nd.waitall()\n",
    "        previous_state = state\n",
    "        # show the frame\n",
    "        renderimage(next_frame)\n",
    "        sample = random.random()\n",
    "        if frame_counter > opt.replay_start_size:\n",
    "            annealing_count += 1\n",
    "        if frame_counter == opt.replay_start_size:\n",
    "            logging.error('annealing and laerning are started ')\n",
    "\n",
    "        data = nd.array(state.reshape([1,opt.frame_len,opt.image_size,opt.image_size]),opt.ctx)\n",
    "        a = nd.dot(E_W_,dqn_(data)[0].T)\n",
    "        action = np.argmax(a.asnumpy()).astype(np.uint8)\n",
    "        \n",
    "        # Skip frame\n",
    "        rew = 0\n",
    "        for skip in range(opt.skip_frame-1):\n",
    "            next_frame, reward, done,_ = env.step(action)\n",
    "            renderimage(next_frame)\n",
    "            cum_clipped_reward += rew_clipper(reward)\n",
    "            rew += reward\n",
    "            for internal_skip in range(opt.internal_skip_frame-1):\n",
    "                _ , reward, done,_ = env.step(action)\n",
    "                cum_clipped_reward += rew_clipper(reward)\n",
    "                rew += reward\n",
    "        next_frame_new, reward, done, _ = env.step(action)\n",
    "        renderimage(next_frame)\n",
    "        cum_clipped_reward += rew_clipper(reward)\n",
    "        rew += reward\n",
    "        cum_reward += rew\n",
    "        \n",
    "        # Reward clipping\n",
    "        reward = rew_clipper(rew)\n",
    "        next_frame = np.maximum(next_frame_new,next_frame)\n",
    "        state = preprocess(next_frame, state)\n",
    "        replay_memory.push((previous_state*255.).astype('uint8')\\\n",
    "                           ,action,(state*255.).astype('uint8'),reward,done)\n",
    "        # Thompson Sampling\n",
    "        if frame_counter % opt.f_sampling:\n",
    "            E_W_ = sample_W(E_W,Cov_W_decom)\n",
    "        \n",
    "        # Train\n",
    "        if frame_counter > opt.replay_start_size: \n",
    "            if frame_counter % opt.learning_frequency == 0:\n",
    "                batch = replay_memory.sample(opt.batch_size)\n",
    "                #update network\n",
    "                for j in range(opt.batch_size):\n",
    "                    batch_state[j] = batch[j].state.as_in_context(opt.ctx).astype('float32')/255.\n",
    "                    batch_state_next[j] = batch[j].next_state.as_in_context(opt.ctx).astype('float32')/255.\n",
    "                    batch_reward[j] = batch[j].reward\n",
    "                    batch_action[j] = batch[j].action\n",
    "                    batch_done[j] = batch[j].done\n",
    "                with autograd.record():\n",
    "                    argmax_Q = nd.argmax(nd.dot(dqn_(batch_state_next),E_W_.T),axis = 1).astype('int32')\n",
    "                    Q_sp_ = nd.dot(target_dqn_(batch_state_next),E_W_target.T)\n",
    "                    Q_sp = nd.pick(Q_sp_,argmax_Q,1) * (1 - batch_done)\n",
    "                    Q_s_array = nd.dot(dqn_(batch_state),E_W.T)\n",
    "                    if (Q_s_array[0,0] != Q_s_array[0,0]).asscalar():\n",
    "                        flag = 1\n",
    "                        print('break')\n",
    "                        break\n",
    "                    Q_s = nd.pick(Q_s_array,batch_action,1)\n",
    "                    loss = nd.mean(l2loss(Q_s ,  (batch_reward + opt.gamma *Q_sp)))\n",
    "                loss.backward()\n",
    "                DQN_trainer.step(opt.batch_size)\n",
    "        t += 1\n",
    "        frame_counter += 1\n",
    "        # Save the model, update Target model and update posterior\n",
    "        if frame_counter > opt.replay_start_size:\n",
    "            if frame_counter % opt.Target_update == 0 :\n",
    "                check_point = frame_counter / (opt.Target_update *100)\n",
    "                fdqn = './data/target_%s_%d' % (env_name,int(check_point))\n",
    "                dqn_.save_params(fdqn)\n",
    "                target_dqn_.load_params(fdqn, opt.ctx)\n",
    "                c_t += 1\n",
    "                if c_t == opt.target_W_update:\n",
    "                    phiphiT,phiY,E_W,Cov_W = BayesReg(phiphiT,phiY,opt.alpha_target,opt.target_batch_size)\n",
    "                    E_W_target = E_W\n",
    "                    Cov_W_target = Cov_W\n",
    "                    fnam = './data/clippted_rew_BDQN_%s_tarUpd_%d_lr_%f' %(env_name,opt.target_W_update,opt.lr)\n",
    "                    np.save(fnam,tot_clipped_reward)\n",
    "                    fnam = './data/tot_rew_BDQN_%s_tarUpd_%d_lr_%f' %(env_name,opt.target_W_update,opt.lr)\n",
    "                    np.save(fnam,tot_reward)\n",
    "                    fnam = './data/frame_count_BDQN_%s_tarUpd_%d_lr_%f' %(env_name,opt.target_W_update,opt.lr)\n",
    "                    np.save(fnam,frame_count_record)\n",
    "                    fnam = './data/E_W_target_BDQN_%s_tarUpd_%d_lr_%f_%d' %(env_name,opt.target_W_update,opt.lr,int(check_point))\n",
    "                    np.save(fnam,E_W_target.asnumpy())\n",
    "                    fnam = './data/Cov_W_target_BDQN_%s_tarUpd_%d_lr_%f_%d' %(env_name,opt.target_W_update,opt.lr,int(check_point))\n",
    "                    np.save(fnam,Cov_W_target.asnumpy())\n",
    "                    \n",
    "                    c_t = 0\n",
    "                    for ii in range(num_action):\n",
    "                        Cov_W_decom[ii] = nd.array(np.linalg.cholesky(((Cov_W[ii]+nd.transpose(Cov_W[ii]))/2.).asnumpy()),ctx = opt.ctx)\n",
    "                if len(replay_memory.memory) < 100000:\n",
    "                    opt.target_batch_size = len(replay_memory.memory)\n",
    "                else:\n",
    "                    opt.target_batch_size = 100000\n",
    "        if done:\n",
    "            if epis_count % 100. == 0. :\n",
    "                logging.error('BDQN:env:%s,epis[%d],durat[%d],fnum=%d, cum_cl_rew = %d, cum_rew = %d,tot_cl = %d , tot = %d'\\\n",
    "                  %(env_name, epis_count,t+1,frame_counter,cum_clipped_reward,cum_reward,moving_average_clipped,moving_average))\n",
    "    epis_count += 1\n",
    "    tot_clipped_reward = np.append(tot_clipped_reward, cum_clipped_reward)\n",
    "    tot_reward = np.append(tot_reward, cum_reward)\n",
    "    frame_count_record = np.append(frame_count_record,frame_counter)\n",
    "    if epis_count > 100.:\n",
    "        moving_average_clipped = np.mean(tot_clipped_reward[int(epis_count)-1-100:int(epis_count)-1])\n",
    "        moving_average = np.mean(tot_reward[int(epis_count)-1-100:int(epis_count)-1])\n",
    "    \n",
    "    if flag:\n",
    "        print('break')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the overall performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_c = tot_clipped_reward\n",
    "tot = tot_reward\n",
    "fram = frame_count_record\n",
    "epis_count = len(fram)\n",
    "\n",
    "\n",
    "bandwidth = 1 # Moving average bandwidth\n",
    "total_clipped = np.zeros(int(epis_count)-bandwidth)\n",
    "total_rew = np.zeros(int(epis_count)-bandwidth)\n",
    "f_num = fram[0:epis_count-bandwidth]\n",
    "\n",
    "\n",
    "for i in range(int(epis_count)-bandwidth):\n",
    "    total_clipped[i] = np.sum(tot_c[i:i+bandwidth])/bandwidth\n",
    "    total_rew[i] = np.sum(tot[i:i+bandwidth])/bandwidth\n",
    "        \n",
    "    \n",
    "t = np.arange(int(epis_count)-bandwidth)\n",
    "belplt = plt.plot(f_num,total_rew[0:int(epis_count)-bandwidth],\"b\", label = \"BDQN\")\n",
    "\n",
    "\n",
    "plt.ticklabel_format(axis='both', style='sci', scilimits=(-2,2),fontsize=fonts, family = 'serif')\n",
    "plt.legend(fontsize=fonts)\n",
    "print('Running after %d number of episodes' %epis_count)\n",
    "plt.xlabel(\"Number of steps\",fontsize=fonts, family = 'serif')\n",
    "plt.ylabel(\"Average Reward per episode\",fontsize=fonts, family = 'serif')\n",
    "plt.title(\"%s\" %(env_name),fontsize=fonts, family = 'serif')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Accumulated average reward compared to DDQN\n",
    "|![](./images/Amidar.png)|![](./images/Assault.png)|\n",
    "|:---------------:|:---------------:|\n",
    "\n",
    "|![](./images/Asteroids.png)|![](./images/Atlantis.png)|\n",
    "|:---------------:|:---------------:|\n",
    "\n",
    "|![](./images/BattleZone.png)|![](./images/Pong.png)|\n",
    "|:---------------:|:---------------:|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limits of Open AI Gym\n",
    "As it is seen for Atlantis game, the reward suddenly saturates. We investigate this effect and realized that it reached the internal limit of OpenAIGym environment, after removing this limit the agent got to score of 62M after seeing 15M samples, which is 100X larger than DDQN. \n",
    "\n",
    "![](./images/Atlantisnolimit.png)\n",
    "\n",
    "As we can see the score drops down after reaching the score of 62M. This was expected since the length of reply buffer is short and the agent fills it with samples of latest part of the game. But we can see that thanks to Thompson Sampling, the agent reach the score of 30M immediately after that where it keeps alternating. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "This article is under review in ICLR\n",
    "@article{\n",
    "  anonymous2018efficient,\n",
    "  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n",
    "  author={Anonymous},\n",
    "  journal={International Conference on Learning Representations},\n",
    "  year={2018},\n",
    "  url={https://openreview.net/forum?id=Bk6qQGWRb}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
