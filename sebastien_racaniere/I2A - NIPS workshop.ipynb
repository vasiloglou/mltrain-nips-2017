{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "I2A - NIPS workshop",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [
        "3ZAuNuy8E2lg"
      ]
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "kcw2prXvB0QK",
        "colab_type": "text"
      },
      "source": [
        "* Builds a feed-forward agent, the I2AAgent.\n",
        "* Includes the Copy-model agent from the paper, where the GTM simply copies frames that are passed in.\n",
        "* Includes a model with size preserving convolutional network.\n",
        "* A fully fledged I2A agent would require a pre-trained environment model.\n",
        "\n",
        "This colab shows how to build the agent, and how to distill the I2A agent policy into its internal rollout policy. This colab does not show how to implement full A3C training with the A2C loss."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "hiWkhj0UCIY5",
        "colab_type": "toc"
      },
      "source": [
        ">>[The PillEater environment](#scrollTo=3ZAuNuy8E2lg&uniqifier=15)\n",
        "\n",
        ">>[The components](#scrollTo=nY3LE6KkCCKR&uniqifier=15)\n",
        "\n",
        ">>[Putting it all together](#scrollTo=R4NTKiOXrHQU&uniqifier=15)\n",
        "\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "PPgmZpt9O8bj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "!pip install dm-sonnet"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cz8J6DM5MXqe",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sonnet as snt\n",
        "import tensorflow as tf\n",
        "import time\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ZAuNuy8E2lg",
        "colab_type": "text"
      },
      "source": [
        "## The PillEater environment"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "KNUaDXN1GoXp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "STANDARD_MAP = np.array([\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "    [1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "    [1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
        "    [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
        "    [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1],\n",
        "    [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
        "    [1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "    [1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
        "\n",
        "\n",
        "def get_random_position(map_array):\n",
        "  \"\"\"Gets a random available position in a binary map array.\n",
        "\n",
        "  Args:\n",
        "    map_array: numpy array of the map to search an available position on.\n",
        "\n",
        "  Returns:\n",
        "    The chosen random position.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if there is no available space in the map.\n",
        "  \"\"\"\n",
        "  if map_array.sum() <= 0:\n",
        "    raise ValueError(\"There is no available space in the map.\")\n",
        "  map_dims = len(map_array.shape)\n",
        "  pos = np.zeros(map_dims, dtype=np.int32)\n",
        "  while True:\n",
        "    result = map_array\n",
        "    for i in range(map_dims):\n",
        "      pos[i] = np.random.randint(map_array.shape[i])\n",
        "      result = result[pos[i]]\n",
        "    if result == 0:\n",
        "      break\n",
        "  return pos\n",
        "\n",
        "\n",
        "def update_2d_pos(array_map, pos, action, pos_result):\n",
        "  posv = array_map[pos[0]][pos[1]][action - 1]\n",
        "  pos_result[0] = posv[0]\n",
        "  pos_result[1] = posv[1]\n",
        "  return pos_result\n",
        "\n",
        "\n",
        "def parse_map(map_array):\n",
        "  \"\"\"Parses a map when there are actions: stay, right, up, left, down.\n",
        "\n",
        "  Args:\n",
        "    map_array: 2D numpy array that contains the map.\n",
        "\n",
        "  Returns:\n",
        "    A 3D numpy array (height, width, actions) that contains the resulting state\n",
        "    for a given position + action, and a 2D numpy array (height, width) with the\n",
        "    walls of the map.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if the map does not contain only zeros and ones.\n",
        "  \"\"\"\n",
        "  act_def = [[0, 0], [0, 1], [-1, 0], [0, -1], [1, 0]]\n",
        "  walls = np.zeros_like(map_array)\n",
        "  new_map_array = []\n",
        "  for i in range(map_array.shape[0]):\n",
        "    new_map_array.append([])\n",
        "    for j in range(map_array.shape[1]):\n",
        "      new_map_array[i].append([])\n",
        "      if map_array[i, j] == 0:\n",
        "        for k in range(len(act_def)):\n",
        "          new_map_array[i][j].append([i + act_def[k][0], j + act_def[k][1]])\n",
        "      elif map_array[i, j] == 1:\n",
        "        for k in range(len(act_def)):\n",
        "          new_map_array[i][j].append([i, j])\n",
        "        walls[i, j] = 1\n",
        "      else:\n",
        "        raise ValueError(\"Option not understood, %d\" % map_array[i, j])\n",
        "      for k in range(len(new_map_array[i][j])):\n",
        "        if map_array[new_map_array[i][j][k][0]][new_map_array[i][j][k][1]] == 1:\n",
        "          new_map_array[i][j][k][0] = i\n",
        "          new_map_array[i][j][k][1] = j\n",
        "  return np.array(new_map_array), walls\n",
        "\n",
        "\n",
        "def observation_as_rgb(obs):\n",
        "  \"\"\"Reduces the 6 channels of `obs` to 3 RGB.\n",
        "\n",
        "  Args:\n",
        "    obs: the observation as a numpy array.\n",
        "\n",
        "  Returns:\n",
        "    An RGB image in the form of a numpy array, with values between 0 and 1.\n",
        "  \"\"\"\n",
        "  height = obs.shape[0]\n",
        "  width = obs.shape[1]\n",
        "  rgb = np.zeros((height, width, 3), dtype=np.float32)\n",
        "  for x in range(height):\n",
        "    for y in range(width):\n",
        "      if obs[x, y, PillEater.PILLMAN] == 1:\n",
        "        rgb[x, y] = [0, 1, 0]\n",
        "      elif obs[x, y, PillEater.GHOSTS] > 0. or obs[x, y, PillEater.GHOSTS_EDIBLE] > 0.:\n",
        "        g = obs[x, y, PillEater.GHOSTS]\n",
        "        ge = obs[x, y, PillEater.GHOSTS_EDIBLE]\n",
        "        rgb[x, y] = [g + ge, ge, 0]\n",
        "      elif obs[x, y, PillEater.PILL] == 1:\n",
        "        rgb[x, y] = [0, 1, 1]\n",
        "      elif obs[x, y, PillEater.FOOD] == 1:\n",
        "        rgb[x, y] = [0, 0, 1]\n",
        "      elif obs[x, y, PillEater.WALLS] == 1:\n",
        "        rgb[x, y] = [1, 1, 1]\n",
        "  return rgb\n",
        "\n",
        "\n",
        "class PillEater(object):\n",
        "\n",
        "  WALLS = 0\n",
        "  FOOD = 1\n",
        "  PILLMAN = 2\n",
        "  GHOSTS = 3\n",
        "  GHOSTS_EDIBLE = 4\n",
        "  PILL = 5\n",
        "  NUM_ACTIONS = 5\n",
        "  MODES = ('regular', 'avoid', 'hunt', 'ambush', 'rush')\n",
        "\n",
        "  def __init__(self, mode, frame_cap=3000):\n",
        "    assert mode in PillEater.MODES\n",
        "    self.nghosts_init = 1\n",
        "    self.ghost_speed_init = 0.5\n",
        "    self.ghost_speed = self.ghost_speed_init\n",
        "    self.ghost_speed_increase = 0.1\n",
        "    self.end_on_collect = False\n",
        "    self.npills = 2\n",
        "    self.pill_duration = 20\n",
        "    self.seed = 123\n",
        "    self.discount = 1\n",
        "    self.stochasticity = 0.05\n",
        "    self.obs_is_rgb = True\n",
        "    self.frame_cap = frame_cap\n",
        "    self.safe_distance = 5\n",
        "    map_array = STANDARD_MAP\n",
        "    self.map, self.walls = parse_map(map_array)\n",
        "    self.map = np.array(self.map)\n",
        "    self.nactions = self.map.shape[2]\n",
        "    self.height = self.map.shape[0]\n",
        "    self.width = self.map.shape[1]\n",
        "    self.reverse_dir = (4, 5, 2, 3)\n",
        "    self.dir_vec = np.array([[0, 1], [-1, 0], [0, -1], [1, 0]])\n",
        "    self.world_state = dict(\n",
        "        pillman=self._make_pillman(),\n",
        "        ghosts=[],\n",
        "        food=np.zeros(shape=(self.height, self.width), dtype=np.float32),\n",
        "        pills=[None] * self.npills,\n",
        "        power=0\n",
        "    )\n",
        "    self.nplanes = 6\n",
        "    self.image = np.zeros(\n",
        "        shape=(self.height, self.width, self.nplanes), dtype=np.float32)\n",
        "    self.color_image = np.zeros(shape=(3, self.height, self.width),\n",
        "                                dtype=np.float32)\n",
        "    self.frame = 0\n",
        "    self.reward = 0.\n",
        "    self.pcontinue = 1.\n",
        "    self._init_level(1)\n",
        "    self._make_image()\n",
        "    self.mode = mode\n",
        "    self.timer = 0\n",
        "    if self.mode == 'regular':\n",
        "      self.step_reward = 0\n",
        "      self.food_reward = 1\n",
        "      self.big_pill_reward = 2\n",
        "      self.ghost_hunt_reward = 5\n",
        "      self.ghost_death_reward = 0\n",
        "      self.all_pill_terminate = False\n",
        "      self.all_ghosts_terminate = False\n",
        "      self.all_food_terminate = True\n",
        "      self.timer_terminate = -1\n",
        "    elif self.mode == 'avoid':\n",
        "      self.step_reward = 0.1\n",
        "      self.food_reward = -0.1\n",
        "      self.big_pill_reward = -5\n",
        "      self.ghost_hunt_reward = -10\n",
        "      self.ghost_death_reward = -20\n",
        "      self.all_pill_terminate = False\n",
        "      self.all_ghosts_terminate = False\n",
        "      self.all_food_terminate = True\n",
        "      self.timer_terminate = 128\n",
        "    elif self.mode == 'hunt':\n",
        "      self.step_reward = 0\n",
        "      self.food_reward = 0\n",
        "      self.big_pill_reward = 1\n",
        "      self.ghost_hunt_reward = 10\n",
        "      self.ghost_death_reward = -20\n",
        "      self.all_pill_terminate = False\n",
        "      self.all_ghosts_terminate = True\n",
        "      self.all_food_terminate = False\n",
        "      self.timer_terminate = -1\n",
        "    elif self.mode == 'ambush':\n",
        "      self.step_reward = 0\n",
        "      self.food_reward = -0.1\n",
        "      self.big_pill_reward = 0\n",
        "      self.ghost_hunt_reward = 10\n",
        "      self.ghost_death_reward = -20\n",
        "      self.all_pill_terminate = False\n",
        "      self.all_ghosts_terminate = True\n",
        "      self.all_food_terminate = False\n",
        "      self.timer_terminate = -1\n",
        "    elif self.mode == 'rush':\n",
        "      self.step_reward = 0\n",
        "      self.food_reward = -0.1\n",
        "      self.big_pill_reward = 10\n",
        "      self.ghost_hunt_reward = 0\n",
        "      self.ghost_death_reward = 0\n",
        "      self.all_pill_terminate = True\n",
        "      self.all_ghosts_terminate = False\n",
        "      self.all_food_terminate = False\n",
        "      self.timer_terminate = -1\n",
        "\n",
        "  def _make_pillman(self):\n",
        "    return self._make_actor(0)\n",
        "\n",
        "  def _make_enemy(self):\n",
        "    return self._make_actor(self.safe_distance)\n",
        "\n",
        "  def _make_actor(self, safe_distance):\n",
        "    \"\"\"Creates an actor.\n",
        "\n",
        "    An actor is a `ConfigDict` with a positions `pos` and a direction `dir`.\n",
        "    The position is an array with two elements, the height and width. The\n",
        "    direction is an integer representing the direction faced by the actor.\n",
        "\n",
        "    Args:\n",
        "      safe_distance: a `float`. The minimum distance from Pillman.\n",
        "\n",
        "    Returns:\n",
        "      A `ConfigDict`.\n",
        "    \"\"\"\n",
        "    actor = {}\n",
        "    if safe_distance > 0:\n",
        "      occupied_map = np.copy(self.walls)\n",
        "\n",
        "      from_ = (self.world_state['pillman']['pos'] - np.array(\n",
        "          [self.safe_distance, self.safe_distance]))\n",
        "      to = (self.world_state['pillman']['pos'] + np.array(\n",
        "          [self.safe_distance, self.safe_distance]))\n",
        "      from_[0] = max(from_[0], 1)\n",
        "      from_[1] = max(from_[1], 1)\n",
        "      to[0] = min(to[0], occupied_map.shape[0])\n",
        "      to[1] = min(to[1], occupied_map.shape[1])\n",
        "\n",
        "      occupied_map[from_[0]:to[0], from_[1]:to[1]] = 1\n",
        "\n",
        "      actor['pos'] = get_random_position(occupied_map)\n",
        "      actor['dir'] = np.random.randint(4)\n",
        "    else:\n",
        "      actor['pos'] = get_random_position(self.walls)\n",
        "      actor['dir'] = np.random.randint(4)\n",
        "\n",
        "    return actor\n",
        "\n",
        "  def _make_pill(self):\n",
        "    pill = dict(\n",
        "        pos=get_random_position(self.walls)\n",
        "    )\n",
        "    return pill\n",
        "\n",
        "  def _init_level(self, level):\n",
        "    \"\"\"Initialises the level.\"\"\"\n",
        "    self.level = level\n",
        "    self._fill_food(self.walls, self.world_state['food'])\n",
        "    self.world_state['pills'] = [self._make_pill() for _ in range(self.npills)]\n",
        "    self.world_state['pillman']['pos'] = get_random_position(self.walls)\n",
        "\n",
        "    self.nghosts = int(self.nghosts_init + math.floor((level - 1) / 2))\n",
        "    self.world_state['ghosts'] = [self._make_enemy() for _ in range(self.nghosts)]\n",
        "    self.world_state['power'] = 0\n",
        "\n",
        "    self.ghost_speed = (\n",
        "        self.ghost_speed_init + self.ghost_speed_increase * (level - 1))\n",
        "    self.timer = 0\n",
        "\n",
        "  def _fill_food(self, walls, food):\n",
        "    food.fill(-1)\n",
        "    food *= walls\n",
        "    food += 1\n",
        "    self.nfood = food.sum()\n",
        "\n",
        "  def _get_food(self, posx, posy):\n",
        "    self.reward += self.food_reward\n",
        "    self.world_state['food'][posx][posy] = 0\n",
        "    self.nfood -= 1\n",
        "    if self.nfood == 0 and self.all_food_terminate:\n",
        "      self._init_level(self.level + 1)\n",
        "\n",
        "  def _get_pill(self, pill_index):\n",
        "    self.world_state['pills'].pop(pill_index)\n",
        "    self.reward += self.big_pill_reward\n",
        "    self.world_state['power'] = self.pill_duration\n",
        "    if (not self.world_state['pills']) and self.all_pill_terminate:\n",
        "      self._init_level(self.level + 1)\n",
        "\n",
        "  def _kill_ghost(self, ghost_index):\n",
        "    self.world_state['ghosts'].pop(ghost_index)\n",
        "    self.reward += self.ghost_hunt_reward\n",
        "    if (not self.world_state['ghosts']) and self.all_ghosts_terminate:\n",
        "      self._init_level(self.level + 1)\n",
        "\n",
        "  def _die_by_ghost(self):\n",
        "    self.reward += self.ghost_death_reward\n",
        "    self.pcontinue = 0\n",
        "\n",
        "  def _move_pillman(self, action):\n",
        "    \"\"\"Moves Pillman following the action in the proto `action_proto`.\"\"\"\n",
        "    action += 1  # our code is 1 based\n",
        "    pos = self.world_state['pillman']['pos']\n",
        "    pillman = self.world_state['pillman']\n",
        "    update_2d_pos(self.map, pos, action, pos)\n",
        "    if self.world_state['food'][pos[0]][pos[1]] == 1:\n",
        "      self._get_food(pos[0], pos[1])\n",
        "    for i, pill in enumerate(self.world_state['pills']):\n",
        "      pos = pill['pos']\n",
        "      if pos[0] == pillman['pos'][0] and pos[1] == pillman['pos'][1]:\n",
        "        self._get_pill(i)\n",
        "        break\n",
        "\n",
        "  def _move_ghost(self, ghost):\n",
        "    \"\"\"Moves the given ghost.\"\"\"\n",
        "    pos = ghost['pos']\n",
        "    new_pos = np.zeros(shape=(2,), dtype=np.float32)\n",
        "    pillman = self.world_state['pillman']\n",
        "    available = []\n",
        "    for i in range(2, self.nactions + 1):\n",
        "      update_2d_pos(self.map, pos, i, new_pos)\n",
        "      if pos[0] != new_pos[0] or pos[1] != new_pos[1]:\n",
        "        available.append(i)\n",
        "    n_available = len(available)\n",
        "    if n_available == 1:\n",
        "      ghost['dir'] = available[0]\n",
        "    elif n_available == 2:\n",
        "      if ghost['dir'] not in available:\n",
        "        if self.reverse_dir[ghost['dir'] - 2] == available[0]:\n",
        "          ghost['dir'] = available[1]\n",
        "        else:\n",
        "          ghost['dir'] = available[0]\n",
        "    else:\n",
        "      rev_dir = self.reverse_dir[ghost['dir'] - 2]\n",
        "      for i in range(n_available):\n",
        "        if available[i] == rev_dir:\n",
        "          available.pop(i)\n",
        "          n_available -= 1\n",
        "          break\n",
        "      prods = np.zeros(n_available, dtype=np.float32)\n",
        "      x = np.array(\n",
        "          [pillman['pos'][0] - pos[0], pillman['pos'][1] - pos[1]], dtype=np.float32)\n",
        "      norm = np.linalg.norm(x)\n",
        "      if norm > 0:\n",
        "        x *= 1. / norm\n",
        "        for i in range(n_available):\n",
        "          prods[i] = np.dot(x, self.dir_vec[available[i] - 2])\n",
        "        if self.world_state['power'] == 0:\n",
        "          if self.stochasticity > np.random.uniform():\n",
        "            j = np.random.randint(n_available)\n",
        "          else:\n",
        "            # move towards pillman:\n",
        "            j = np.argmax(prods)\n",
        "        else:\n",
        "          # run away from pillman:\n",
        "          j = np.argmin(prods)\n",
        "        ghost['dir'] = available[j]\n",
        "    update_2d_pos(self.map, pos, ghost['dir'], pos)\n",
        "\n",
        "  def _make_image(self):\n",
        "    \"\"\"Represents world in a `height x width x 6` `Tensor`.\"\"\"\n",
        "    self.image.fill(0)\n",
        "    self.image[:, :, PillEater.WALLS] = self.walls\n",
        "    self.image[:, :, PillEater.FOOD] = self.world_state['food']\n",
        "    self.image[self.world_state['pillman']['pos'][0], self.world_state['pillman']['pos'][1],\n",
        "               PillEater.PILLMAN] = 1\n",
        "    for ghost in self.world_state['ghosts']:\n",
        "      edibility = self.world_state['power'] / float(self.pill_duration)\n",
        "      self.image[ghost['pos'][0], ghost['pos'][1], PillEater.GHOSTS] = 1. - edibility\n",
        "      self.image[ghost['pos'][0], ghost['pos'][1], PillEater.GHOSTS_EDIBLE] = edibility\n",
        "    for pill in self.world_state['pills']:\n",
        "      self.image[pill['pos'][0], pill['pos'][1], PillEater.PILL] = 1\n",
        "    return self.image\n",
        "\n",
        "  def start(self):\n",
        "    \"\"\"Starts a new episode.\"\"\"\n",
        "    self.frame = 0\n",
        "    self._init_level(1)\n",
        "    self.reward = 0\n",
        "    self.pcontinue = 1\n",
        "    self.ghost_speed = self.ghost_speed_init\n",
        "    return self._make_image(), self.reward, self.pcontinue\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Advances environment one time-step following the given action.\"\"\"\n",
        "    self.frame += 1\n",
        "    pillman = self.world_state['pillman']\n",
        "    self.pcontinue = self.discount\n",
        "    self.reward = self.step_reward\n",
        "    self.timer += 1\n",
        "    # Update world state\n",
        "    self.world_state['power'] = max(0, self.world_state['power']-1)\n",
        "\n",
        "    # move pillman\n",
        "    self._move_pillman(action)\n",
        "\n",
        "    for i, ghost in enumerate(self.world_state['ghosts']):\n",
        "      # first check if pillman went onto a ghost\n",
        "      pos = ghost['pos']\n",
        "      if pos[0] == pillman['pos'][0] and pos[1] == pillman['pos'][1]:\n",
        "        if self.world_state['power'] == 0:\n",
        "          self._die_by_ghost()\n",
        "        else:\n",
        "          self._kill_ghost(i)\n",
        "          break\n",
        "      # Then move ghosts\n",
        "      speed = self.ghost_speed\n",
        "      if self.world_state['power'] != 0:\n",
        "        speed *= 0.5\n",
        "      if np.random.uniform() < speed:\n",
        "        self._move_ghost(ghost)\n",
        "        pos = ghost['pos']\n",
        "        # check if ghost went onto pillman\n",
        "        if pos[0] == pillman['pos'][0] and pos[1] == pillman['pos'][1]:\n",
        "          if self.world_state['power'] == 0:\n",
        "            self._die_by_ghost()\n",
        "          else:\n",
        "            self._kill_ghost(i)\n",
        "            # assume you can only eat one ghost per turn:\n",
        "            break\n",
        "    self._make_image()\n",
        "\n",
        "    # Check if level over\n",
        "    if self.timer == self.timer_terminate:\n",
        "      self._init_level(self.level + 1)\n",
        "\n",
        "    # Check if framecap reached\n",
        "    if self.frame_cap > 0 and self.frame >= self.frame_cap:\n",
        "      self.pcontinue = 0\n",
        "\n",
        "  def observation(self, agent_id=0):\n",
        "    return (self.reward,\n",
        "            self.pcontinue,\n",
        "            observation_as_rgb(self.image))\n",
        "\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nY3LE6KkCCKR",
        "colab_type": "text"
      },
      "source": [
        "## The components"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "Hy0obCpXHfvU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "def update_frame_stack(old_stack, new_frame):\n",
        "  return tf.concat([old_stack[..., 3:], new_frame], axis=-1)\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bj4MovHWMczy",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "-"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "class I2AAgent(snt.RNNCore):\n",
        "  \"\"\"The I2A agent imagines possible futures and learns how to interpret these\n",
        "  imaginations.\n",
        "  \n",
        "  The state of the agent is made of the last few frames observed from the\n",
        "  environment. This state is initialised to black frames.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_actions, model_free_path, imag_path,\n",
        "               height, width, stack_size, name='i2a_agent'):\n",
        "    super(I2AAgent, self).__init__(name=name)\n",
        "    self.num_actions = num_actions\n",
        "    self.model_free_path = model_free_path\n",
        "    self.imag_path = imag_path\n",
        "    self.height = height\n",
        "    self.width = width\n",
        "    self.stack_size = stack_size\n",
        "    \n",
        "  def _build(self, frame, prev_state):\n",
        "    # Add a batch dimension\n",
        "    frame = tf.expand_dims(frame, axis=0)\n",
        "    next_state = update_frame_stack(prev_state, frame)\n",
        "    \n",
        "    # Compute features from imagination and model free paths\n",
        "    imag_feature = self.imag_path(next_state)\n",
        "    model_free_feature = self.model_free_path(next_state)\n",
        "    policy_feature = tf.concat([imag_feature, model_free_feature], axis=1)\n",
        "\n",
        "    # Compute logits and baseline (needed for A2C loss)\n",
        "    value_and_logits = snt.Linear(\n",
        "        output_size=1 + self.num_actions,\n",
        "        name='value_and_logits')(policy_feature)\n",
        "    baseline=value_and_logits[:, 0]\n",
        "    policy_logits=value_and_logits[:, 1:]\n",
        "    action = tf.multinomial(policy_logits, 1)\n",
        "    action = tf.cast(action, tf.int32)\n",
        "    return (action, policy_logits, baseline), next_state\n",
        "  \n",
        "  def initial_state(self, batch_size=1):\n",
        "    assert batch_size == 1\n",
        "    return tf.zeros((batch_size, self.height, self.width, 3*self.stack_size))\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "63HW22E5OMlK",
        "colab_type": "code",
        "slideshow": {
          "slide_type": "slide"
        },
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "class FrameProcessing(snt.AbstractModule):\n",
        "  \"\"\"Can be used for model-free path for example. This module outputs a flat\n",
        "  Tensor.\"\"\"\n",
        "  \n",
        "  def __init__(self, output_size,  name='frame_processing'):\n",
        "    super(FrameProcessing, self).__init__(name=name)\n",
        "    self.output_size = output_size\n",
        "    \n",
        "  def _build(self, frame):\n",
        "    hidden = snt.Conv2D(\n",
        "        output_channels=16, kernel_shape=3, stride=1)(frame)\n",
        "    hidden = tf.nn.relu(hidden)\n",
        "    hidden = snt.Conv2D(\n",
        "        output_channels=16, kernel_shape=3, stride=2)(hidden)\n",
        "    hidden = tf.nn.relu(hidden)\n",
        "    hidden = snt.Linear(self.output_size)(snt.BatchFlatten()(hidden))\n",
        "    return tf.nn.relu(hidden)\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ldpVKNLPvxO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "class ImagPath(snt.AbstractModule):\n",
        "  \"\"\"Imagines possible futures and encodes all imaginations into a single flat\n",
        "  Tensor.\"\"\"\n",
        "  \n",
        "  def __init__(self, num_actions, rollout_depth,\n",
        "               env_model, rollout_policy, single_imag_feature,\n",
        "               name='imag_path'):\n",
        "    super(ImagPath, self).__init__(name=name)\n",
        "    self.num_actions = num_actions\n",
        "    self.rollout_depth = rollout_depth\n",
        "    self.env_model = env_model\n",
        "    self.rollout_policy = rollout_policy\n",
        "    self.single_imag_feature = single_imag_feature\n",
        "    \n",
        "  def _tile_by_actions(self, tensor):\n",
        "    return tf.concat(\n",
        "        [tf.expand_dims(tensor, axis=1)] * self.num_actions, axis=1)\n",
        "\n",
        "  def _build(self, frame_stack):\n",
        "    imag_features = []\n",
        "    # We need to do 'num_actions' rollouts. For efficiency reasons, we need to\n",
        "    # batch all rollouts. If frame_stack has a batch dimension of B, then we\n",
        "    # will have an effective batch dimension of B * num_actions.\n",
        "    batch_size = frame_stack.get_shape()[0].value\n",
        "    # Tile frame_stack to have shape (B, num_actions, height, width, channels).\n",
        "    frame_stack = self._tile_by_actions(frame_stack)\n",
        "    # We force the first action\n",
        "    action = tf.constant(range(self.num_actions),\n",
        "                         shape=(self.num_actions,),\n",
        "                         dtype=tf.int32)\n",
        "    action = tf.stack([action] * batch_size)\n",
        "    # Use snt.BatchApply to collapse the first two dimensions into a single\n",
        "    # dimension.\n",
        "    env_model = snt.BatchApply(self.env_model)\n",
        "    imag_feature = snt.BatchApply(self.single_imag_feature)\n",
        "    frame, reward, _ = env_model(frame_stack, action)\n",
        "    frame_stack = update_frame_stack(frame_stack, frame)\n",
        "    imag_features.append(imag_feature(frame, reward, action))\n",
        "    # Subsequent actions come from the rollout policy.\n",
        "    for _ in range(self.rollout_depth - 1):\n",
        "      action, _ = snt.BatchApply(self.rollout_policy)(frame)\n",
        "      frame, reward, _ = env_model(frame_stack, action)\n",
        "      frame_stack = update_frame_stack(frame_stack, frame)\n",
        "      imag_features.append(imag_feature(frame, reward, action))\n",
        "    # Process all imagination features in reverse order to encode the rollouts\n",
        "    lstm = snt.LSTM(256)\n",
        "    lstm_state = [self._tile_by_actions(t) for t in lstm.initial_state(batch_size)]\n",
        "    for feature in imag_features[::-1]:\n",
        "      lstm_output, lstm_state = snt.BatchApply(lstm)(feature, lstm_state)\n",
        "    encoded_rollouts = lstm_output\n",
        "    # Flatten\n",
        "    return tf.reshape(encoded_rollouts, [1, -1])\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ua05Vpx-CsSM",
        "colab_type": "text"
      },
      "source": [
        "Either of the models below can be used in I2A. The first model implements one of the\n",
        "baselines considered in the paper."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "__mjCMTLgqi9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "class SizePreservingConvNetModel(snt.AbstractModule):\n",
        "  \"\"\"A simple model that predicts the next frame using a size preserving\n",
        "  convolutional network. The action is tiled and broadcasted to the whole\n",
        "  input frame stack as additional channels.\"\"\"\n",
        "  \n",
        "  def __init__(self, num_actions, reward_bins,\n",
        "               name='size_preserving_conv_net_model'):\n",
        "    super(SizePreservingConvNetModel, self).__init__(name=name)\n",
        "    self.num_actions = num_actions\n",
        "    self.num_reward_bins = len(reward_bins)\n",
        "    self.reward_bins = tf.constant(\n",
        "        reward_bins, shape=(1, len(reward_bins)), dtype=tf.float32)\n",
        "\n",
        "  def _build(self, frame_stack, action):\n",
        "    # One-hot actions are broadcasted to all locations in the frames.\n",
        "    action_one_hot = tf.one_hot(action, depth=self.num_actions)\n",
        "    action_2d = tf.expand_dims(action_one_hot, axis=1)\n",
        "    action_2d = tf.expand_dims(action_2d, axis=1)\n",
        "    height, width = [d.value for d in frame_stack.get_shape()[1:3]]\n",
        "    action_tiled = snt.TileByDim(dims=[1, 2], multiples=[height, width])(\n",
        "        action_2d)\n",
        "    conv_input = tf.concat([frame_stack, action_tiled], axis=3)\n",
        "    hidden = snt.Conv2D(\n",
        "        output_channels=16, kernel_shape=3, stride=1)(conv_input)\n",
        "    hidden = tf.nn.relu(hidden)\n",
        "    # Rewards are binned to transform the problem of learning a reward predictor\n",
        "    # into a classification problem.\n",
        "    hidden_for_reward = snt.Conv2D(\n",
        "        output_channels=8, kernel_shape=3, stride=2)(hidden)\n",
        "    hidden_for_reward = tf.nn.relu(hidden_for_reward)\n",
        "    hidden_for_reward = snt.Linear(self.num_reward_bins)(\n",
        "        snt.BatchFlatten()(hidden_for_reward)\n",
        "    )\n",
        "    reward_bins = tf.nn.softmax(hidden_for_reward)\n",
        "    reward = tf.reduce_sum(reward_bins * self.reward_bins, axis=1)\n",
        "    # Pixels are treated as Bernoulli variables.\n",
        "    hidden_for_frame = snt.Conv2D(\n",
        "        output_channels=3, kernel_shape=3, stride=1)(hidden)\n",
        "    frame = tf.nn.sigmoid(hidden_for_frame)\n",
        "    return frame, reward, reward_bins\n",
        "\n",
        "\n",
        "class CopyModel(snt.AbstractModule):\n",
        "  \"\"\"A dummy model that ignores the action and outputs the input frame.\"\"\"\n",
        "  \n",
        "  def __init__(self, num_reward_bins, name='copy_model'):\n",
        "    super(CopyModel, self).__init__(name=name)\n",
        "    self.num_reward_bins = num_reward_bins\n",
        "    \n",
        "  def _build(self, frame_stack, action):\n",
        "    dummy_reward = tf.constant(\n",
        "        0, shape=(frame_stack.get_shape()[0],), dtype=tf.float32)\n",
        "    dummy_reward_bins = tf.constant(\n",
        "        0, shape=(frame_stack.get_shape()[0], self.num_reward_bins), dtype=tf.float32)\n",
        "    last_frame = frame_stack[:, :, :, -3:]\n",
        "    return last_frame, dummy_reward, dummy_reward_bins\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-CRKFAjh3Fc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "class SingleImagFeature(snt.AbstractModule):\n",
        "  \"\"\"This transforms a tuple of (imagined frame, predicted reward, action)\n",
        "  into a feature.\"\"\"\n",
        "  \n",
        "  def __init__(self, output_size, num_actions, name='single_imag_feature'):\n",
        "    super(SingleImagFeature, self).__init__(name=name)\n",
        "    self.output_size = output_size\n",
        "    self.num_actions = num_actions\n",
        "  \n",
        "  def _build(self, frame, reward, action):\n",
        "    frame_feature = FrameProcessing(self.output_size)(frame)\n",
        "    action_one_hot = tf.one_hot(action, depth=self.num_actions)\n",
        "    return tf.concat(\n",
        "        [frame_feature, tf.expand_dims(reward, axis=1), action_one_hot],\n",
        "        axis=1)\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OllwVFZMqOwO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "class RolloutPolicy(snt.AbstractModule):\n",
        "  \"\"\"The rollout policy maps imagined frames to actions.\"\"\"\n",
        "  \n",
        "  def __init__(self, num_actions, name='rollout_policy'):\n",
        "    super(RolloutPolicy, self).__init__(name=name)\n",
        "    self.num_actions = num_actions\n",
        "    \n",
        "  def _build(self, frame):\n",
        "    frame_feature = FrameProcessing(128)(frame)\n",
        "    logits = snt.Linear(\n",
        "        output_size=self.num_actions,\n",
        "        name='logits')(frame_feature)\n",
        "    action = tf.multinomial(logits, 1)\n",
        "    action = tf.cast(action, tf.int32)\n",
        "    # We need to output the logits to be able to distill another policy\n",
        "    # into our rollout policy\n",
        "    return action[:, 0], logits\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R4NTKiOXrHQU",
        "colab_type": "text"
      },
      "source": [
        "## Putting it all together"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "bnYF6XuYrKdN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "9a791971-e586-4ea6-c0a5-5f27881212a5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512585227849,
          "user_tz": 480,
          "elapsed": 3188,
          "user": {
            "displayName": "SÃ©bastien Racaniere",
            "photoUrl": "//lh4.googleusercontent.com/-7lP1T3HsCAw/AAAAAAAAAAI/AAAAAAAAABY/fWvmCeoJXTk/s50-c-k-no/photo.jpg",
            "userId": "102775271233982205447"
          }
        }
      },
      "source": [
        "def run():\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  num_actions = 5\n",
        "  rollout_depth = 3\n",
        "  output_size = 128\n",
        "  height = 15\n",
        "  width = 19\n",
        "  rgb = 3\n",
        "  frame_cap = 39\n",
        "  learning_rate = 1e-5\n",
        "  stack_size = 4\n",
        "  env_model_type = CopyModel\n",
        "\n",
        "  # Model free path\n",
        "  model_free_path = FrameProcessing(output_size)\n",
        "  # Imagination path\n",
        "  if env_model_type == CopyModel:\n",
        "    env_model = CopyModel(num_reward_bins=3)\n",
        "  elif env_model_type == SizePreservingConvNetModel:\n",
        "    env_model = SizePreservingConvNetModel(num_actions=num_actions,\n",
        "                                           reward_bins=(-1., 0., 1.))\n",
        "  rollout_policy = RolloutPolicy(num_actions)\n",
        "  single_imag_feature = SingleImagFeature(output_size, num_actions)\n",
        "  imag_path = ImagPath(\n",
        "      num_actions, rollout_depth,\n",
        "      env_model, rollout_policy, single_imag_feature)\n",
        "  # The I2A agent\n",
        "  i2a_agent = I2AAgent(num_actions, model_free_path, imag_path,\n",
        "                       height=height, width=width, stack_size=stack_size)\n",
        "\n",
        "  input_frame = tf.placeholder(\n",
        "      shape=(height, width, rgb), dtype=tf.float32, name='input_frame')\n",
        "  # We connect the agent to the initial state. We can override this by using\n",
        "  # a feed_dict with agent_state as key at session runtime.\n",
        "  agent_state = i2a_agent.initial_state()\n",
        "  (action, policy_logits, baseline), agent_next_state = i2a_agent(input_frame,\n",
        "                                                                  agent_state)\n",
        "  # The environment\n",
        "  env = PillEater(mode='regular', frame_cap=frame_cap)\n",
        "  env.start()\n",
        "\n",
        "  # Distillation loss. Sonnet handles weight sharing naturally, so we can simply\n",
        "  # apply the rollout_policy on the frames.\n",
        "  _, rollout_logits = rollout_policy(tf.expand_dims(input_frame, axis=0))\n",
        "  distill_loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "      logits=rollout_logits,\n",
        "      labels=tf.stop_gradient(tf.nn.softmax(policy_logits)))\n",
        "  optim_step = tf.train.RMSPropOptimizer(\n",
        "      learning_rate=learning_rate, epsilon=0.1).minimize(distill_loss)\n",
        "\n",
        "  # Run until termination of an episode, and display all observed frames.\n",
        "  init_global = tf.global_variables_initializer()\n",
        "  tf.get_default_graph().finalize()\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init_global)\n",
        "\n",
        "    env_pcontinue = 1.\n",
        "    num_frames = frame_cap + 1\n",
        "    num_horiz = 10\n",
        "    num_vert = (num_frames + num_horiz - 1) // num_horiz\n",
        "    fig, axes_grid = plt.subplots(num_vert, num_horiz)\n",
        "    fig.set_figwidth(2 * num_horiz)\n",
        "    fig.set_figheight(2 * num_vert)\n",
        "    axes = []\n",
        "    for ax in axes_grid:\n",
        "      axes.extend(ax)\n",
        "    for axis in axes:\n",
        "      axis.get_xaxis().set_visible(False)\n",
        "      axis.get_yaxis().set_visible(False)\n",
        "    step_count = 0\n",
        "    total_reward = 0.\n",
        "    prev_state = None\n",
        "    while env_pcontinue == 1.:\n",
        "      env_reward, env_pcontinue, env_frame = env.observation()\n",
        "      total_reward += env_reward\n",
        "      feed_dict = {input_frame: env_frame}\n",
        "      if prev_state is not None:\n",
        "        feed_dict[agent_state] = prev_state\n",
        "      action_out, _, prev_state = sess.run([action, optim_step, agent_next_state],\n",
        "                                             feed_dict=feed_dict)\n",
        "      env.step(action_out[0, 0])\n",
        "      axis = axes[step_count]\n",
        "      axis.imshow(env_frame)\n",
        "      step_count += 1\n",
        "    print('Total reward at end of episode: {}.'.format(total_reward))\n",
        "\n",
        "run()"
      ],
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total reward at end of episode: 7.0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAAGtCAYAAABOYpKtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAF8tJREFUeJzt3MFu61oORUG6cf//l9WDDJ7hHCdy\nJEebTNXwQnHCt+BugBB427ZtKwAAAAAu97+r/wAAAAAAPljUAAAAAISwqAEAAAAIYVEDAAAAEMKi\nBgAAACCERQ0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQljUAAAAAIT49+oP3G6f/22rz/94q+3z\nc5//afl56c6eY/V576Zj/457/9ZV19Ufq2HOd3FFx2Of904a9m9YpWNV/44a9m9Ydf7fq2Pud3FF\nw/2f9246XtvRGzUAAAAAISxqAAAAAEJY1AAAAACEsKgBAAAACPHyMeGl5ZWdcz/uKn/qiJKOS506\nrv/WxUHoA593lb/SsErHZzp11HCtU8MqHZ/p1FHDtU4Nq578d1/+wfsC6fj7NFzr1LBKx2fe0dEb\nNQAAAAAhLGoAAAAAQljUAAAAAISwqAEAAAAIcdu21074JB836uqKI0o6nu+3O2p4Pt/FGXwX+/Nd\nnMF3sT/fxRl8F/vzXZzhlY7eqAEAAAAIYVEDAAAAEMKiBgAAACCERQ0AAABAiH9nfMjqKM7q+NCR\n565y9hzJR5l07N9Rw/4Nq3Q8+lwCDfs3rNLx6HMJNOzfsErHo88l0LB/wyodjz73Cm/UAAAAAISw\nqAEAAAAIYVEDAAAAEMKiBgAAACDEKceE99p7UCf5gFJV/t/3bjr2p+EMOvan4Qw69qfhDDr2p+EM\nOh7njRoAAACAEBY1AAAAACEsagAAAABCWNQAAAAAhHjbMeFt+/nPro72HPm89N+bTMf+NJxBx/40\nnEHH/jScQcf+NJxBx/fwRg0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQpxyTHh1jGeKybM9mjzr\n5NnuTZ5z8myPJs86ebZ7k+ecPNujybNOnu3e5Dknz/Zo8qyTZ7s3ec7Jsz2aPGvabN6oAQAAAAhh\nUQMAAAAQwqIGAAAAIIRFDQAAAECIU44Jb9sZn/KfpEM+k2d7NHnWybPdmzzn5NkeTZ518mz3Js85\nebZHk2edPNu9yXNOnu3R5Fknz3Zv8pyTZ3s0eda02bxRAwAAABDCogYAAAAghEUNAAAAQAiLGgAA\nAIAQrx8T3nlkZ3k8Z/WzO4/sXHVo6MjvPfsg0al03C22o4a7xTas0vEFsR013C22YZWOL4jt+Nca\nrv7A277/CLENq/5ex9UvXs7x+R9jO2rYv2GVjlWXdvRGDQAAAEAIixoAAACAEBY1AAAAACEsagAA\nAABCvH5MeO+xoJ2PLT8u6KjS8qbQ4u+76ujRj+nYv6OG/RtW6VgDOv61hou/cFtM0qphlY41oONf\nazjxf0+rdKwn38Vf+FtOo2H/hlU61rUdvVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBC3bXvt\nhE+7g2QNXHFEScfz/XZHDc/nu/gLtsXAt3P/w/suvtuB64I7+S7+hnkdNazq3rBKxw+9O2pY1b1h\nlY4fruvojRoAAACAEBY1AAAAACEsagAAAABCWNQAAAAAhPh3xoesjuKsjg8dee4qt8VRoW1xVOjI\nvCl07N9Rw/4Nq4Z3PHmO1I4a9m9YpePR5xJo2L9hlY5Hn0ugYf+GVToefe4V3qgBAAAACGFRAwAA\nABDCogYAAAAghEUNAAAAQIhTjgmfffAo64DS4jLQzqNHWXN8T8e1rDm+puFa1hzf+3Mdd8qa42sa\nrmXN8T0d17Lm+JqGa1lzfE/Htaw5vqbhWtYc39Nx7R1zeKMGAAAAIIRFDQAAAEAIixoAAACAEBY1\nAAAAACFu27b9/GoOAAAAAKfxRg0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQljUAAAAAISwqAEA\nAAAIYVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQljUAAAAAISwqAEA\nAAAIYVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQljUAAAAAISwqAEA\nAAAIYVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQljUAAAAAISwqAEA\nAAAIYVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQljUAAAAAISwqAEA\nAAAIYVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQljUAAAAAISwqAEA\nAAAIYVEDAAAAEMKiBgAAACDEv1d/4Hb7+S/btnM/7ypnz7H6vHfTsX9HDfs3rNKxqn9HDfs3rNKx\nqn9HDfs3rNKxqn9HDfs3rNKx6tqO3qgBAAAACGFRAwAAABDCogYAAAAghEUNAAAAQIiXjwmvTDiW\n9MxfOqKk41qnjhqudWpYpeMznTpquNapYZWOz3TqqOFap4ZVOj7TqaOGa50aVun4zDs6eqMGAAAA\nIIRFDQAAAEAIixoAAACAEBY1AAAAACFu2/baCZ+9R3G2+vzgrYKuBQW54ojSoWNJ2i79dsezD43p\n2u+7uKKj7+IEvoszdP8urvy1rhO+i3tNbjvxu7ii4bmSjhpPaftKR2/UAAAAAISwqAEAAAAIYVED\nAAAAEMKiBgAAACDEvzM+ZHUU53b7/I/r5/Z93lX2/n1Hnkux++9dPrjvsav8lY7HZtr3nb3KX2lY\npePR5xJo2L9hlY5Hn0tw+kyLB6/K+lcaVp0/1/q7eE3bv9JRw/4Nq36r4/I37/iXY9I6eqMGAAAA\nIIRFDQAAAEAIixoAAACAEBY1AAAAACFOOSa8196DOskHlKry/75307E/DWfQsT8NZ9CxPw1n0LE/\nDWfQ8Thv1AAAAACEsKgBAAAACGFRAwAAABDCogYAAAAgxNuOCW/bz392dbTnyOel/95kOvan4Qw6\n9qfhDDr2p+EMOvan4Qw6voc3agAAAABCWNQAAAAAhLCoAQAAAAhhUQMAAAAQ4pRjwqtjPFNMnu3R\n5Fknz3Zv8pyTZ3s0edbJs92bPOfk2R5NnnXybPcmzzl5tkeTZ508273Jc06e7dHkWdNm80YNAAAA\nQAiLGgAAAIAQFjUAAAAAISxqAAAAAEKcckx42874lP8kHfKZPNujybNOnu3e5Dknz/Zo8qyTZ7s3\nec7Jsz2aPOvk2e5NnnPybI8mzzp5tnuT55w826PJs6bN5o0aAAAAgBAWNQAAAAAhLGoAAAAAQljU\nAAAAAIR4/ZjwziM7y+M5q5/deWTnqkNDt9UvXs7x+R/PPkh0Kh37d9Swf8MqHav6d9Swf8MqHav6\nd9Swf8MqHav6d9Swf8MqHasu7eiNGgAAAIAQFjUAAAAAISxqAAAAAEJY1AAAAACEeP2Y8N5jQTsf\nW35c0FGl9U2hz3/gRTePfk7H/h017N+wSsca0FHD/g2rdKwBHTXs37BKxxrQUcP+Dat0rGs7eqMG\nAAAAIIRFDQAAAEAIixoAAACAEBY1AAAAACFu2/baCZ/VkZ3ZDlxR2umKI0o6VnXvqGFV94ZVOn7o\n3VHDqu4Nq3T80LujhlXdG1bp+KF3Rw2rujes0vHDdR29UQMAAAAQwqIGAAAAIIRFDQAAAEAIixoA\nAACAEP/O+JDVUZzV8aEjz13l7DmSjzLp2L+jhv0bVul49LkEGvZvWKXj0ecSaNi/YZWOR59LoGH/\nhlU6Hn3uFd6oAQAAAAhhUQMAAAAQwqIGAAAAIIRFDQAAAECIU44Jn33wKOuA0s8vHGXN8T0d17Lm\n+JqGa1lzfE/Htaw5vqbhWtYc39NxLWuOr2m4ljXH93Rcy5rjaxquZc3xPR3X3jGHN2oAAAAAQljU\nAAAAAISwqAEAAAAIYVEDAAAAEOK2bdvPr+YAAAAAcBpv1AAAAACEsKgBAAAACGFRAwAAABDCogYA\nAAAghEUNAAAAQAiLGgAAAIAQFjUAAAAAISxqAAAAAEJY1AAAAACEsKgBAAAACGFRAwAAABDCogYA\nAAAghEUNAAAAQAiLGgAAAIAQFjUAAAAAISxqAAAAAEJY1AAAAACEsKgBAAAACGFRAwAAABDCogYA\nAAAghEUNAAAAQAiLGgAAAIAQFjUAAAAAISxqAAAAAEJY1AAAAACEsKgBAAAACGFRAwAAABDCogYA\nAAAghEUNAAAAQAiLGgAAAIAQFjUAAAAAISxqAAAAAEJY1AAAAACEsKgBAAAACGFRAwAAABDCogYA\nAAAghEUNAAAAQAiLGgAAAIAQFjUAAAAAISxqAAAAAEJY1AAAAACEsKgBAAAACGFRAwAAABDCogYA\nAAAghEUNAAAAQAiLGgAAAIAQFjUAAAAAIf69+gO3289/2bad+3lXOXuO1ee9m479O2rYv2GVjlX9\nO2rYv2GVjlX9O2rYv2GVjlX9O2rYv2GVjlXXdvRGDQAAAEAIixoAAACAEBY1AAAAACEsagAAAABC\nvHxMeGXCsaRn/tIRJR3XOnXUcK1Twyodn+nUUcO1Tg2rdHymU0cN1zo1rNLxmU4dNVzr1LBKx2fe\n0dEbNQAAAAAhLGoAAAAAQljUAAAAAISwqAEAAAAIcdu21074JB83qqra6vMfeKugK0ULVxxR0vF8\nv91Rw/P5Ln6m4/c0PJ/v4mc6fi+94Up6V9/Fn0tq67v4M3+5YZWO7/BKR2/UAAAAAISwqAEAAAAI\nYVEDAAAAEMKiBgAAACDEvzM+ZHUUZ3V86MhzL/w1O/5lv7PnSD7KpGP/jhr2b1il49HnEmjYv2GV\njkefS5DVcOXnXf9Kw6oOHVf2tf0rHTXs37BKx6PPvcIbNQAAAAAhLGoAAAAAQljUAAAAAISwqAEA\nAAAIccox4b32HtRJPqBUlf/3vZuO/Wk4g479aTiDjv1pOIOO/Wk4g47HeaMGAAAAIIRFDQAAAEAI\nixoAAACAEBY1AAAAACHedkx4237+s6ujPUc+L/33JtOxPw1n0LE/DWfQsT8NZ9CxPw1n0PE9vFED\nAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBCnHBNeHeOZYvJsjybPOnm2e5PnnDzbo8mzTp7t3uQ5\nJ8/2aPKsk2e7N3nOybM9mjzr5NnuTZ5z8myPJs+aNps3agAAAABCWNQAAAAAhLCoAQAAAAhhUQMA\nAAAQ4pRjwtt2xqf8J+mQz+TZHk2edfJs9ybPOXm2R5NnnTzbvclzTp7t0eRZJ892b/Kck2d7NHnW\nybPdmzzn5NkeTZ41bTZv1AAAAACEsKgBAAAACGFRAwAAABDCogYAAAAgxOvHhHce2Vkez1n97M4j\nO1cdGrqtfvFyjs//ePZBolPpuLaYbdv7H+u3abjblvxl1HHNd/Fnn/cLRjas0vGZTh01XOvUsErH\nZzp11HCtU8MqHZ/5pY7eqAEAAAAIYVEDAAAAEMKiBgAAACCERQ0AAABAiNePCe89FrTzseXHBd1U\nWt8S/vwHXnTz6Od0XB6Y3X0MK4GG64bdvo06Pvku7jzknkDD/g2rdKwBHTXs37BKxxrQUcP+Dat0\nrGs7eqMGAAAAIIRFDQAAAEAIixoAAACAEBY1AAAAACFu2+pCzlc/0OxO53Hvvy57xRElHau6d9Sw\nqnvDKh0/9O6oYVX3hlU6fujdUcOq7g2rdPzQu6OGVd0bVun44bqO3qgBAAAACGFRAwAAABDCogYA\nAAAghEUNAAAAQIh/Z3zI6ijO6vjQkeeucvYcyUeZdOzfUcP+Dat0PPpcAg37N6zS8ehzCTTs37BK\nx6PPJdCwf8MqHY8+9wpv1AAAAACEsKgBAAAACGFRAwAAABDCogYAAAAgxCnHhM8+eJR1QOnnF46y\n5viejmtZc3xNw7WsOb6n41rWHF/TcC1rju/puJY1x9c0XMua43s6rmXN8TUN17Lm+J6Oa++Ywxs1\nAAAAACEsagAAAABCWNQAAAAAhLCoAQAAAAhx27bt51dzAAAAADiNN2oAAAAAQljUAAAAAISwqAEA\nAAAIYVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQljUAAAAAISwqAEA\nAAAIYVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQljUAAAAAISwqAEA\nAAAIYVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQljUAAAAAISwqAEA\nAAAIYVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQljUAAAAAISwqAEA\nAAAIYVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQljUAAAAAISwqAEA\nAAAIYVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBAWNQAAAAAh/r36A7fbz3/Ztp37eVc5e47V\n572bjv07ati/YZWOVf07ati/YZWOVf07ati/YZWOVf07ati/YZWOVdd29EYNAAAAQAiLGgAAAIAQ\nFjUAAAAAISxqAAAAAEK8fEx4ZcKxpGf+0hElHdc6ddRwrVPDKh2f6dRRw7VODat0fKZTRw3XOjWs\n0vGZTh01XOvUsErHZ97R0Rs1AAAAACEsagAAAABCWNQAAAAAhLCoAQAAAAhx27bXTvgkHzc6aqvP\nw93q/ReOrjiiNLnjym+0/e2OGvZvWPX3Ov4G38X+fBdn8F3sz3dxBt/F/nwXZ3ilozdqAAAAAEJY\n1AAAAACEsKgBAAAACGFRAwAAABDi3xkfsjqKszo+dOS537H6AxdPnTxvijkdP7vdPv8xEztq2L9h\n1eyOe//A7h017N+wanbHs+dI7ahh/4ZVOh59LoGG/RtW6Xj0uVd4owYAAAAghEUNAAAAQAiLGgAA\nAIAQFjUAAAAAIU45JrzX3oM6yQeUqvL/vnfTsT8NZ+jZcd/R9r9Cwxk6dtwW0VZH2/8KDWfQsT8N\nZ9DxOG/UAAAAAISwqAEAAAAIYVEDAAAAEMKiBgAAACDE244Jbwfu7qyOCh35vPTfm0zH/jScQcf+\nNJxhSsfVL/4raTWcQcf+NJxBx/fwRg0AAABACIsaAAAAgBAWNQAAAAAhLGoAAAAAQpxyTHh5jGeI\nybM9mjzr5NnuTZ5z8myPJs86ebZ7k+ecPNujybNOnu3e5Dknz/Zo8qyTZ7s3ec7Jsz2aPGvabN6o\nAQAAAAhhUQMAAAAQwqIGAAAAIIRFDQAAAECIU44Jb9sZn/KfpEM+k2d7NHnWybPdmzzn5NkeTZ51\n8mz3Js85ebZHk2edPNu9yXNOnu3R5Fknz3Zv8pyTZ3s0eda02bxRAwAAABDCogYAAAAghEUNAAAA\nQAiLGgAAAIAQrx8T3nlkZ3k8Z/WzO4/sXHVo6Lb3Fy9m2/b+x7qCjmudOmq41qlhlY7PdOqo4Vqn\nhlU6vmA7++LiWTTcLbZhlY4viO2o4Zr/X/z55/2CtI7eqAEAAAAIYVEDAAAAEMKiBgAAACCERQ0A\nAABAiNePCe89FrTzseXHBd1UWt0UWh3uWh4fCprjEx37d9Swf8MqHWtARw37N6zSsZ503Hv9MYGG\n/RtW6VgDOmro/xf3flzQ/GkdvVEDAAAAEMKiBgAAACCERQ0AAABACIsaAAAAgBC3bXUh56sfaHTH\n6hwHrijtdMURJR2runfUsKp7wyodP/TuqGFV94ZVOn7o3VHDqu4Nq3T80LujhlXdG1bp+OG6jt6o\nAQAAAAhhUQMAAAAQwqIGAAAAIIRFDQAAAECIf2d8yOoozur40JHnrnL2HMlHmXTs31HD/g2rdDz6\nXAIN+zes0vHocwk07N+wSsejzyXQsH/DKh2PPvcKb9QAAAAAhLCoAQAAAAhhUQMAAAAQwqIGAAAA\nIMQpx4TPPniUdUDp5xeOsub4no5rWXN8TcO1rDm+p+Na1hxf03Ata47v6biWNcfXNFzLmuN7Oq5l\nzfE1Ddey5viejmvvmMMbNQAAAAAhLGoAAAAAQljUAAAAAISwqAEAAAAIcdu27edXcwAAAAA4jTdq\nAAAAAEJY1AAAAACEsKgBAAAACGFRAwAAABDCogYAAAAghEUNAAAAQAiLGgAAAIAQFjUAAAAAISxq\nAAAAAEJY1AAAAACEsKgBAAAACPF/SOAVIQb3KX0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f649f279ad0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}