{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Attention model for Visual Question Answering in MXNET Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this notebook, we introduce attention from <a href=\"https://arxiv.org/pdf/1606.01847.pdf\">\"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\"</a>, Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell and Marcus Rohrbach, EMNLP 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each spatial grid location in the visual representation, we use count sketch to merge the slice of the visual feature with the language representation. After the pooling we use two convolutional layers to predict the attention weight for each grid location. We apply softmax to produce a normalized soft attention map. We then take a weighted sum of the spatial vectors using the attention map to create the attended visual representation. \n",
    "\n",
    "![](img/VQA-attention.png )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import mxnet.ndarray as F\n",
    "import mxnet.contrib.ndarray as C\n",
    "import mxnet.gluon as gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd\n",
    "import bisect\n",
    "from IPython.core.display import display, HTML\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import os\n",
    "from mxnet.test_utils import download\n",
    "import json\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "ctx = mx.gpu(2)\n",
    "#ctx = mx.cpu()\n",
    "compute_size  = batch_size\n",
    "out_dim = 10000\n",
    "gpus = 1\n",
    "class Net(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Net, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.bn = nn.BatchNorm()\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            self.fc1 = nn.Dense(8192,activation=\"relu\")\n",
    "            self.fc2 = nn.Dense(1000)\n",
    "            self.conv1 = nn.Conv2D(channels=512, strides=(1,1), padding=(1, 1), kernel_size=3, activation='relu',layout = 'NCHW',use_bias=False)\n",
    "            self.conv2 = nn.Conv2D(channels=1, strides=(1,1), padding=(1, 1), kernel_size=3, activation='relu',layout = 'NCHW',use_bias=False)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x1 = F.L2Normalization(x[0])\n",
    "        x2 = F.L2Normalization(x[1])\n",
    "        \n",
    "        # Reshape the inputs\n",
    "        text_data_copy = F.Reshape(x1, shape=(0,0,1,1))\n",
    "        text_data_copy = F.broadcast_to(text_data_copy,shape=(0,0,14,14))\n",
    "        text_swapaxis = F.transpose(data = text_data_copy, axes=(0,2,3,1)) #from N 1024 14(H) 14  to N 14(H) 14 1024\n",
    "        img_swapaxis = F.transpose(data = x2, axes=(0,2,3,1)) #from N 1024 14(H) 14  to N 14(H) 14 1024\n",
    "        # Count sketch\n",
    "        S1 = F.array(np.random.randint(0, 2, (1,1024))*2-1,ctx = ctx)\n",
    "        H1 = F.array(np.random.randint(0, out_dim,(1,1024)),ctx = ctx)\n",
    "        S2 = F.array(np.random.randint(0, 2, (1,2048))*2-1,ctx = ctx)\n",
    "        H2 = F.array(np.random.randint(0, out_dim,(1,2048)),ctx = ctx)\n",
    "        cs1 = C.count_sketch( data = text_swapaxis,s = S1, h= H1,name= 'cs1',out_dim = out_dim,processing_batch_size = compute_size) \n",
    "        cs2 = C.count_sketch( data = img_swapaxis,s = S2, h = H2,name='cs2',out_dim = out_dim,processing_batch_size = compute_size)\n",
    "        fft1 = C.fft(data = cs1, name='fft1', compute_size =compute_size) \n",
    "        fft2 = C.fft(data = cs1, name='fft2', compute_size =compute_size) \n",
    "        c = fft2*fft1\n",
    "        ifft = C.ifft(data = c, name='ifft1', compute_size = compute_size) \n",
    "        # Attention\n",
    "        mcb0_swapaxis = F.transpose(data = ifft, axes=(0,3,1,2)) #from N 14(H) 14 5000 to N 5000 14(H) 14\n",
    "        cv0 = self.conv1(mcb0_swapaxis)\n",
    "        cv1 = self.conv2(cv0)\n",
    "        body = F.SoftmaxActivation(data=cv1, mode = \"channel\", name='softmax0')  #N 1 14(H) 14 \n",
    "        image_attention = F.broadcast_mul(x2,body)\n",
    "        image_attention = F.sum(image_attention,axis = (2,3))   #N 2048\n",
    "        # CSecond ount sketch\n",
    "        S3 = F.array(np.random.randint(0, 2, (1,1024))*2-1,ctx = ctx)\n",
    "        H3 = F.array(np.random.randint(0, out_dim,(1,1024)),ctx = ctx)\n",
    "        S4 = F.array(np.random.randint(0, 2, (1,2048))*2-1,ctx = ctx)\n",
    "        H4 = F.array(np.random.randint(0, out_dim,(1,2048)),ctx = ctx)\n",
    "        cs3 = C.count_sketch( data = x1,s = S3, h= H3,name= 'cs3',out_dim = out_dim,processing_batch_size = compute_size) \n",
    "        cs4 = C.count_sketch( data = image_attention,s = S4, h = H4,name='cs4',out_dim = out_dim,processing_batch_size = compute_size)\n",
    "        fft3 = C.fft(data = cs3, name='fft3', compute_size =compute_size) \n",
    "        fft4 = C.fft(data = cs4, name='fft4', compute_size = compute_size) \n",
    "        c2 = fft3*fft4\n",
    "        z = C.ifft(data = c2, name='ifft2', compute_size = compute_size) \n",
    "        \n",
    "        #z = F.concat(x1,image_attention,dim=1)\n",
    "        z = self.fc1(z)\n",
    "        z = self.bn(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc2(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The inputs of the data iterator are extracted image and question features. At each step, the data iterator will return a data batch list: question data batch and image data batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from VQAtrainIter import VQAtrainIter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this model, since we need to include the attention scheme, the image feature is a $2048 \\times 14 \\times 14$ tensor, extracted from Resnet-152 pool5. We have 1000 training samples and 100 validation samples for this model due to space limit in this tutorial.\n",
    "\n",
    "We will discuss about how to extract the features <a href=\"extract-feature.ipynb\">here</a> in details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset_files = {'train_attention': ('train_question_attention.npz','train_img_attention.npz','train_ans_attention.npz'),\n",
    "                 'validation_attention': ('val_question_attention.npz','val_img_attention.npz','val_ans_attention.npz')\n",
    "                }\n",
    "train_q, train_i, train_a = dataset_files['train_attention']\n",
    "val_q, val_i, val_a = dataset_files['validation_attention']\n",
    "\n",
    "url_format = 'https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/VQA-notebook/{}'\n",
    "if not os.path.exists(train_q):\n",
    "    logging.info('Downloading training dataset.')\n",
    "    download(url_format.format(train_q),overwrite=True)\n",
    "    download(url_format.format(train_i),overwrite=True)\n",
    "    download(url_format.format(train_a),overwrite=True)\n",
    "if not os.path.exists(val_q):\n",
    "    logging.info('Downloading validation dataset.')\n",
    "    download(url_format.format(val_q),overwrite=True)\n",
    "    download(url_format.format(val_i),overwrite=True)\n",
    "    download(url_format.format(val_a),overwrite=True)\n",
    "\n",
    "layout = 'NT'\n",
    "bucket = [1024]\n",
    "\n",
    "train_question = np.load(train_q)['x']\n",
    "val_question = np.load(val_q)['x']\n",
    "train_ans = np.load(train_a)['x']\n",
    "val_ans = np.load(val_a)['x']\n",
    "train_img = np.load(train_i)['x']\n",
    "val_img = np.load(val_i)['x']\n",
    "\n",
    "print(\"Total training sample:\",train_ans.shape[0])   \n",
    "print(\"Total validation sample:\",val_ans.shape[0])   \n",
    "\n",
    "data_train  = VQAtrainIter(train_img, train_question, train_ans, batch_size, buckets = bucket,layout=layout)\n",
    "data_eva = VQAtrainIter(val_img, val_question, val_ans, batch_size, buckets = bucket,layout=layout) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    \n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        with autograd.record():\n",
    "            data1 = batch.data[0].as_in_context(ctx)\n",
    "            data2 = batch.data[1].as_in_context(ctx)\n",
    "            data = [data1,data2]\n",
    "            label = batch.label[0].as_in_context(ctx)\n",
    "            #label_one_hot = nd.one_hot(label, 10)\n",
    "            output = net(data)\n",
    "        \n",
    "        metric.update([label], [output])\n",
    "    return metric.get()[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "moving_loss = 0.\n",
    "best_eva = 0\n",
    "for e in range(epochs):\n",
    "    data_train.reset()\n",
    "    for i, batch in enumerate(data_train):\n",
    "        data1 = batch.data[0].as_in_context(ctx)\n",
    "        data2 = batch.data[1].as_in_context(ctx)\n",
    "        data = [data1,data2]\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data[0].shape[0])\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        if i == 0:\n",
    "            moving_loss = np.mean(cross_entropy.asnumpy()[0])\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * np.mean(cross_entropy.asnumpy()[0])\n",
    "        #if i % 200 == 0:\n",
    "        #    print(\"Epoch %s, batch %s. Moving avg of loss: %s\" % (e, i, moving_loss))   \n",
    "    eva_accuracy = evaluate_accuracy(data_eva, net)\n",
    "    train_accuracy = evaluate_accuracy(data_train, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Eval_acc %s\" % (e, moving_loss, train_accuracy, eva_accuracy))\n",
    "    if eva_accuracy > best_eva:\n",
    "            best_eva = eva_accuracy\n",
    "            logging.info('Best validation acc found. Checkpointing...')\n",
    "            net.save_params('vqa-mlp-%d.params'%(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
